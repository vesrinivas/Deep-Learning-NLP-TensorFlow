{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srvv\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "##---------------------------------------------------------------\n",
    "## Summary : Implement encoder-decoder using greedy matching\n",
    "## Author  : Srinivas Venkata Vemparala\n",
    "##----------------------------------------------------------------\n",
    "\n",
    "# source https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    \n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [context, utterance]\n",
    "def ExtractData(path, num_examples=None):\n",
    "    lines = open(path, encoding='UTF-8').read().strip().lower().split('\\n')\n",
    "    \n",
    "    if(num_examples==None):\n",
    "        sent_pairs = [[preprocess_sentence(w).split(' ') for w in l.split('\\t')]  for l in lines]\n",
    "    else:\n",
    "        sent_pairs = [[preprocess_sentence(w).split(' ') for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "    \n",
    "    return sent_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class to create the language index. A language index is a mapping from words to integers\n",
    "# and integers to words for all different words\n",
    "class LanguageIndex:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w2i = defaultdict(lambda:len(self.w2i))\n",
    "        self.PAD = self.w2i['<pad>']\n",
    "        self.UNK = self.w2i['<unk>']\n",
    "        \n",
    "    def createIndexMapping(self,listOfSentences):\n",
    "        retList = []\n",
    "        for sent in listOfSentences:\n",
    "            words = [self.w2i[word] for word in sent] \n",
    "            retList.append(words)\n",
    "                \n",
    "        return retList\n",
    "    \n",
    "    def createReverseIndex(self):\n",
    "        # freeze the list so that if new word comes it will be treated as UNK\n",
    "        self.w2i = defaultdict(lambda:self.UNK,self.w2i)\n",
    "        \n",
    "        self.i2w = {v:k for k,v in self.w2i.items()}\n",
    "        \n",
    "        # compute the vocabulary size\n",
    "        self.vocabSize = len(self.w2i)\n",
    "        print('VocabularySize : ',self.vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VocabularySize :  68239\n"
     ]
    }
   ],
   "source": [
    "# now lets write a method to create dataset given path\n",
    "def createDataSet(fileName,num_examples = 30000):\n",
    "    sentPairs = ExtractData(fileName,num_examples)\n",
    "    \n",
    "    # seperate the input and output Data\n",
    "    inputData = [context for context, utterance in sentPairs]\n",
    "    outputData = [utterance for context,utterance in sentPairs]\n",
    "    \n",
    "    # create a languageIndex object and convert the words to numbers\n",
    "    languageIndex = LanguageIndex()\n",
    "    inputIndexed = languageIndex.createIndexMapping(inputData)\n",
    "    outputIndexed = languageIndex.createIndexMapping(outputData)\n",
    "    \n",
    "    # create the inverse index and freeze the vocabulary i..e rest words will be treated as UNK\n",
    "    languageIndex.createReverseIndex()\n",
    "    \n",
    "    # get the max length of the input and output\n",
    "    MAX_LENGTH_INPUT = max(len(sent) for sent in inputData)\n",
    "    MAX_LENGTH_OUTPUT = max(len(sent) for sent in outputData)\n",
    "    \n",
    "    # now let's perform padding for input and output sentences. Padding helps for batching and \n",
    "    # faster processing we are padding post and reversing it so that better accuracy can be \n",
    "    # obtained while decoding ex: ['how','are','you'] after padding for \n",
    "    # len 5 becomes ['<pad>','<pad>','you','are','how']\n",
    "    inputPadded = tf.keras.preprocessing.sequence.pad_sequences(inputIndexed,\n",
    "                                                                maxlen=MAX_LENGTH_INPUT,\n",
    "                                                                padding='post')\n",
    "    inputPadded = np.flip(inputPadded,1) \n",
    "    \n",
    "    # lets pad output. We need to pad it post but no need to reverse it.\n",
    "    # ex:['i','am','fine'] after padding for len 5 becomes ['i','am','fine','<pad>','<pad>']\n",
    "    outputPadded = tf.keras.preprocessing.sequence.pad_sequences(outputIndexed,\n",
    "                                                                maxlen=MAX_LENGTH_OUTPUT,\n",
    "                                                                padding='post') \n",
    "    \n",
    "    return inputPadded,outputPadded,languageIndex,MAX_LENGTH_INPUT,MAX_LENGTH_OUTPUT\n",
    "    \n",
    "\n",
    "# lets get the Dataset\n",
    "inputPadded,outputPadded,languageIndex,MAX_LENGTH_INPUT,MAX_LENGTH_OUTPUT = createDataSet('./Twitter data/Dialogue_Question_Answer.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training Data :  24000\n",
      "length of testing Data :  6000\n"
     ]
    }
   ],
   "source": [
    "# Now we will split the input data into train and test data 80-20 split\n",
    "inputPaddedTrain, inputPaddedTest, outputPaddedTrain, outputPaddedTest = train_test_split(inputPadded, outputPadded, test_size=0.2)\n",
    "\n",
    "print('length of training Data : ',len(inputPaddedTrain))\n",
    "print('length of testing Data : ',len(inputPaddedTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets create a tf.data dataset\n",
    "BUFFER_SIZE = len(inputPaddedTrain)\n",
    "BATCH_SIZE = 64\n",
    "NUM_BATCHES = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 512\n",
    "\n",
    "vocabSize = languageIndex.vocabSize\n",
    "\n",
    "# create the dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputPaddedTrain, outputPaddedTrain)).shuffle(BUFFER_SIZE)\n",
    "\n",
    "# after creating N_batches if few examples are present we drop them\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets define gru which we will be using for encoder and decoder\n",
    "def gru(numUnits):\n",
    "    # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "    # the code automatically does that. glorot_uniform samples weights from\n",
    "    # [-limit,limit] where limit = sqrt(6/fan_in+fan_out)\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(numUnits, \n",
    "                                        return_sequences=True, \n",
    "                                        return_state=True, \n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(numUnits, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True, \n",
    "                                   recurrent_activation='sigmoid', \n",
    "                                   recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets define an encoder class this extends the tf.Keras.Model\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,vocabSize,embedding_dim,enc_units,batchSize):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batchSize\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocabSize, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    # let's implement the call method\n",
    "    def call(self,x,hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    # let's write a method to initialize the hidden state to zeros\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets write the Decoder class. This class also extends the tf.keras.Model\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocabSize, embedding_dim, dec_units, batchSize):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batchSize\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocabSize, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocabSize)\n",
    "        \n",
    "    # let's implement the call method\n",
    "    def call(self,x,hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        return output,state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's create the encoder and decoder\n",
    "encoder = Encoder(vocabSize, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocabSize, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets define the optimizer\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "# loss function\n",
    "def loss_function(real, pred):\n",
    "    # if the real label is PAD then we will consider the loss from it\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.9017\n",
      "Epoch 1 Batch 100 Loss 1.5327\n",
      "Epoch 1 Batch 200 Loss 1.7658\n",
      "Epoch 1 Batch 300 Loss 1.7106\n",
      "Epoch 1 Loss 1.7568\n",
      "Time taken for 1 epoch 12045.129220724106 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.5567\n",
      "Epoch 2 Batch 100 Loss 1.5638\n",
      "Epoch 2 Batch 200 Loss 1.7031\n",
      "Epoch 2 Batch 300 Loss 1.6888\n",
      "Epoch 2 Loss 1.5519\n",
      "Time taken for 1 epoch 13097.710629701614 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.4765\n",
      "Epoch 3 Batch 100 Loss 1.4417\n",
      "Epoch 3 Batch 200 Loss 1.3797\n",
      "Epoch 3 Batch 300 Loss 1.6113\n",
      "Epoch 3 Loss 1.4444\n",
      "Time taken for 1 epoch 7319.031521320343 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.3812\n",
      "Epoch 4 Batch 100 Loss 1.3784\n",
      "Epoch 4 Batch 200 Loss 1.4308\n",
      "Epoch 4 Batch 300 Loss 1.5519\n",
      "Epoch 4 Loss 1.3315\n",
      "Time taken for 1 epoch 7279.965874671936 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.2284\n",
      "Epoch 5 Batch 100 Loss 1.1166\n",
      "Epoch 5 Batch 200 Loss 1.1341\n",
      "Epoch 5 Batch 300 Loss 1.3431\n",
      "Epoch 5 Loss 1.1994\n",
      "Time taken for 1 epoch 7249.232221841812 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.0355\n",
      "Epoch 6 Batch 100 Loss 0.9736\n",
      "Epoch 6 Batch 200 Loss 1.0014\n",
      "Epoch 6 Batch 300 Loss 1.0552\n",
      "Epoch 6 Loss 1.0493\n",
      "Time taken for 1 epoch 7350.941496372223 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.8021\n",
      "Epoch 7 Batch 100 Loss 0.9081\n",
      "Epoch 7 Batch 200 Loss 0.8743\n",
      "Epoch 7 Batch 300 Loss 1.0044\n",
      "Epoch 7 Loss 0.8955\n",
      "Time taken for 1 epoch 16677.11599111557 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.8696\n",
      "Epoch 8 Batch 100 Loss 0.8249\n",
      "Epoch 8 Batch 200 Loss 0.7109\n",
      "Epoch 8 Batch 300 Loss 0.9191\n",
      "Epoch 8 Loss 0.7620\n",
      "Time taken for 1 epoch 10692.349432468414 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.5792\n",
      "Epoch 9 Batch 100 Loss 0.5427\n",
      "Epoch 9 Batch 200 Loss 0.6670\n",
      "Epoch 9 Batch 300 Loss 0.6478\n",
      "Epoch 9 Loss 0.6508\n",
      "Time taken for 1 epoch 14328.374646186829 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.6139\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            # the hidden state of decoder is initialized to the encoder hidden state\n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([languageIndex.w2i['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "                \n",
    "                realLabels = targ[:,t]\n",
    "                \n",
    "                loss += loss_function(realLabels, predictions)\n",
    "                # using teacher forcing. Get the correct word from the training data\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / NUM_BATCHES))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now lets write a method to get the response given the input utterances\n",
    "def getResponse(sentence):\n",
    "    # preprocess the sentence\n",
    "    preprocessed = [preprocess_sentence(sentence).split(' ')] \n",
    "    \n",
    "    # convert the words to int\n",
    "    indexed = languageIndex.createIndexMapping(preprocessed)\n",
    "    \n",
    "    # pad the sentences\n",
    "    inputPadded = tf.keras.preprocessing.sequence.pad_sequences(indexed,\n",
    "                                                                maxlen=MAX_LENGTH_INPUT,\n",
    "                                                                padding='post')\n",
    "    inputPadded = np.flip(inputPadded,1)[0]\n",
    "        \n",
    "    inputPadded = tf.expand_dims(tf.convert_to_tensor(inputPadded),0)\n",
    "    \n",
    "    #initialize the hidden state\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    encoderOutput,encoderHiddenState = encoder(inputPadded,hidden)\n",
    "    \n",
    "    # the hidden state of decoder is initialized to the encoder hidden state\n",
    "    decHidden = encoderHiddenState\n",
    "        \n",
    "    decInput = tf.expand_dims([languageIndex.w2i['<start>']] , 0)\n",
    "    \n",
    "    #initialize output\n",
    "    output = ''\n",
    "    \n",
    "    \n",
    "    for i in range(0,MAX_LENGTH_OUTPUT):\n",
    "        predictions, decHidden = decoder(decInput, decHidden)\n",
    "        \n",
    "        # greedy decoding\n",
    "        predictedWordIndex = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        # non-greedy decoding\n",
    "        #predictedWordIndex = tf.multinomial(predictions, num_samples=1)[0][0].numpy() \n",
    "        \n",
    "        predictedWord = languageIndex.i2w[predictedWordIndex]\n",
    "        \n",
    "        output += predictedWord+' '\n",
    "        \n",
    "        # if end is encountered then stop. \n",
    "        if(predictedWord == '<end>'):\n",
    "            break\n",
    "        decInput = tf.expand_dims([predictedWordIndex], 0)\n",
    "\n",
    "    print(output)    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brightyujin filosofiachaeng im so soft <end> \n"
     ]
    }
   ],
   "source": [
    "getResponse('Learn, connect and explore the future of making things at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sageymoore i love you <end> \n"
     ]
    }
   ],
   "source": [
    "getResponse('I am happy.')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# List of tasks to improve the code\n",
    "1. We are using twitter data which contains a lot of handles like '@xyz' and hashtags like '#feelingxyz'. Removing these will minimize generating redundant replies.\n",
    "2. We are also using different embeddings for encoder and decoder layer. We can use the same embeddings which will lead to better embeddings and lesser parameters.\n",
    "3. We can use beam search to improve the accuracy of the o/p generation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
